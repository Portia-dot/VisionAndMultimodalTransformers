{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-26T01:03:37.072544Z",
     "start_time": "2025-11-26T01:03:15.823877Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T01:03:37.105268Z",
     "start_time": "2025-11-26T01:03:37.081079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CLIPConfig:\n",
    "    def __init__(self):\n",
    "        self.embed_dim = 512\n",
    "        self.num_head = 8\n",
    "        self.dropout = 0.1\n",
    "        self.temperature = 0.7\n",
    "        self.batch_size = 64\n",
    "        self.epochs = 10\n",
    "        self.lr = 1e-4\n",
    "        self.weight_decay = 1e-5\n",
    "        self.warmup_steps = 500\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.log_interval = 50\n",
    "        self.eval_interval = 2\n",
    "        self.use_amp = True\n",
    "\n",
    "config = CLIPConfig()\n",
    "\n"
   ],
   "id": "8d7c338362f9fc06",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T01:03:37.112212Z",
     "start_time": "2025-11-26T01:03:37.109685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim = 2048, dropout = 0.1):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ],
   "id": "c1f73ea0132e0a4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T01:03:37.121431Z",
     "start_time": "2025-11-26T01:03:37.118078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CLIP, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        weight = ResNet50_Weights.IMAGENET1K_V2\n",
    "        self.image_encoder = resnet50(weights = weight)\n",
    "        self.image_encoder.fc = nn.Identity()\n",
    "        self.image_projection = ProjectionHead(in_dim= 2048, out_dim= config.embed_dim, dropout= config.dropout)\n",
    "\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.text_encoder = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.text_projection = ProjectionHead(in_dim= 768, out_dim=config.embed_dim, dropout= config.dropout)\n",
    "\n",
    "        #Tempertature\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/ config.temperature))\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        features = self.image_encoder(images)\n",
    "        projected = self.image_projection(features)\n",
    "        return F.normalize(projected, dim = -1)\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors = \"pt\", padding = True, truncation = True, max_length = 77)\n",
    "        input_ids = inputs[\"input_ids\"].to(self.config.device)\n",
    "        attention_mask = inputs['attention_mask'].to(self.config.device)\n",
    "\n",
    "        outputs = self.text_encoder(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        projected = self.text_projection(pooled_output)\n",
    "        return F.normalize(projected, dim = -1)\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        images_features = self.encode_image(images)\n",
    "        text_features = self.encode_text(texts)\n",
    "        return images_features, text_features\n",
    "\n",
    "\n"
   ],
   "id": "7aa36d953721aec1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T01:03:37.129133Z",
     "start_time": "2025-11-26T01:03:37.124465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_logits(image_features, text_features, logit_scale):\n",
    "    return logit_scale.exp() * image_features @ text_features"
   ],
   "id": "19c9233fc03d90da",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T01:03:37.138266Z",
     "start_time": "2025-11-26T01:03:37.132494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def contrastive_loss(logits):\n",
    "    labels = torch.arange(len(logits), device=logits.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i2t + loss_t2i) / 2\n"
   ],
   "id": "5254f155c517f593",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T01:03:37.147883Z",
     "start_time": "2025-11-26T01:03:37.142047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def recall_at_k(sim_matrix, query_indices, k=5):\n",
    "    top_k = np.min((k, sim_matrix.shape[1]))\n",
    "    top_k_indices = np.argpartition(sim_matrix, -top_k, axis=1)[:, -top_k:]\n",
    "    recalls = []\n",
    "    for i, true_idx in enumerate(query_indices):\n",
    "        if true_idx in top_k_indices[i]:\n",
    "            recalls.append(1)\n",
    "        else:\n",
    "            recalls.append(0)\n",
    "    return np.mean(recalls)"
   ],
   "id": "e71ab991117c1e2d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T01:03:37.156223Z",
     "start_time": "2025-11-26T01:03:37.152346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, split=\"train\", transform=None):\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                std=[0.26862954, 0.26130258, 0.27577711]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # Load 2-column CSV: image, caption\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df.columns = [\"ImageName\", \"Comment\"]  # normalize names\n",
    "\n",
    "        # Image-level split (no leakage)\n",
    "        unique_imgs = df[\"ImageName\"].unique()\n",
    "        np.random.seed(42)\n",
    "        train_imgs = np.random.choice(unique_imgs, int(0.95 * len(unique_imgs)), replace=False)\n",
    "        val_imgs = np.setdiff1d(unique_imgs, train_imgs)\n",
    "\n",
    "        if split == \"train\":\n",
    "            self.df = df[df[\"ImageName\"].isin(train_imgs)].reset_index(drop=True)\n",
    "        else:\n",
    "            self.df = df[df[\"ImageName\"].isin(val_imgs)].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row[\"ImageName\"])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            image = Image.new(\"RGB\", (224, 224), color=\"gray\")\n",
    "\n",
    "        image = self.transform(image)\n",
    "        caption = row[\"Comment\"]\n",
    "\n",
    "        return image, caption\n"
   ],
   "id": "ca6194c228eb4509",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T01:03:37.162631Z",
     "start_time": "2025-11-26T01:03:37.159303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WarmupLinearSchedule(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            lr_mult = float(self.last_epoch) / float(max(1, self.warmup_steps))\n",
    "        else:\n",
    "            lr_mult = 1.0 - float(self.last_epoch - self.warmup_steps) / float(max(1, self.total_steps - self.warmup_steps))\n",
    "        return [base_lr * lr_mult for base_lr in self.base_lrs]"
   ],
   "id": "2a49a70899096b7f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T01:03:37.169785Z",
     "start_time": "2025-11-26T01:03:37.165640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_retrieval(model, val_dataloader, k_values=[1, 5, 10]):\n",
    "    model.eval()\n",
    "    all_image_features = []\n",
    "    all_text_features = []\n",
    "    all_true_indices = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions in tqdm(val_dataloader, desc='Encoding for eval'):\n",
    "            images = images.to(config.device)\n",
    "            i_feats = model.encode_image(images)\n",
    "            all_image_features.append(i_feats.cpu())\n",
    "\n",
    "            t_feats = model.encode_text(captions)\n",
    "            all_text_features.append(t_feats.cpu())\n",
    "            all_true_indices.extend([i for i in range(len(captions))])  # Assume paired\n",
    "\n",
    "    all_image_features = torch.cat(all_image_features)\n",
    "    all_text_features = torch.cat(all_text_features)\n",
    "\n",
    "    # Compute similarities (image-to-text)\n",
    "    logits = (all_image_features @ all_text_features.T).numpy()\n",
    "\n",
    "    recalls = {}\n",
    "    for k in k_values:\n",
    "        recalls[f'R@{k} (i2t)'] = recall_at_k(logits, all_true_indices, k=k)\n",
    "        # Symmetric t2i\n",
    "        recalls[f'R@{k} (t2i)'] = recall_at_k(logits.T, all_true_indices, k=k)\n",
    "\n",
    "    return recalls\n",
    "\n",
    "\n",
    "def train_clip(model, train_loader, val_loader, config):\n",
    "    device = config.device\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    total_steps = len(train_loader) * config.epochs\n",
    "    scheduler = WarmupLinearSchedule(optimizer, config.warmup_steps, total_steps)\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.epochs}')\n",
    "\n",
    "        for batch_idx, (images, captions) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            captions = list(captions)  # List of strings\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            image_features, text_features = model(images, captions)\n",
    "            logits = compute_logits(image_features, text_features, model.logit_scale)\n",
    "            loss = contrastive_loss(logits)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.4f}', 'LR': f'{scheduler.get_last_lr()[0]:.2e}'})\n",
    "\n",
    "            if global_step % config.log_interval == 0:\n",
    "                print(f'Step {global_step}: Loss = {loss.item():.4f}')\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}')\n",
    "\n",
    "        if (epoch + 1) % config.eval_interval == 0:\n",
    "            recalls = evaluate_retrieval(model, val_loader)\n",
    "            print(f'Eval Metrics: {recalls}')\n",
    "\n",
    "    return model"
   ],
   "id": "785596fc4d2af73f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T01:03:37.710391Z",
     "start_time": "2025-11-26T01:03:37.176206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "id": "949b0e53ca933ff1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
      "Path to dataset files: C:\\Users\\modam\\.cache\\kagglehub\\datasets\\adityajn105\\flickr8k\\versions\\1\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-26T01:03:37.717335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "root = r\"C:\\Users\\modam\\.cache\\kagglehub\\datasets\\adityajn105\\flickr8k\\versions\\1\"\n",
    "\n",
    "train_dataset = Flickr8kDataset(\n",
    "    csv_file=f\"{root}\\\\captions.txt\",\n",
    "    image_dir=f\"{root}\\\\Images\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "val_dataset = Flickr8kDataset(\n",
    "    csv_file=f\"{root}\\\\captions.txt\",\n",
    "    image_dir=f\"{root}\\\\Images\",\n",
    "    split=\"val\"\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
    "\n",
    "model = CLIP(config)\n",
    "\n",
    "trained_model = train_clip(model, train_loader, val_loader, config)\n",
    "\n",
    "torch.save({\n",
    "        'model_state_dict': trained_model.state_dict(),\n",
    "        'config': config.__dict__,\n",
    "        'logit_scale': trained_model.logit_scale.item()\n",
    "    }, 'flickr30k_clip_pretrained.pth')\n",
    "print(\"Flickr30k pretraining complete. Model saved as 'flickr30k_clip_pretrained.pth'.\")"
   ],
   "id": "9e4fbb974a0fd642",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 38430, Val size: 2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/19215 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from PIL import Image\n",
    "\n",
    "def clip_inference(model, image_path, candidate_texts, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "            std=[0.26862954, 0.26130258, 0.27577711]\n",
    "        )\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_feat = model.encode_image(image)\n",
    "        txt_feat = model.encode_text(candidate_texts)\n",
    "\n",
    "    sims = (img_feat @ txt_feat.T).squeeze(0)\n",
    "\n",
    "    scores, indices = torch.sort(sims, descending=True)\n",
    "    for rank, idx in enumerate(indices):\n",
    "        print(f\"{rank+1}. {candidate_texts[idx]}    (score={scores[rank]:.4f})\")\n",
    "\n",
    "    return candidate_texts[indices[0]]\n"
   ],
   "id": "d915ce2dbb3e458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a45f5bcd583ea11c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

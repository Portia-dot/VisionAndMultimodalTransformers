{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Implementation of DEIT from scratch",
   "id": "bd6ea848f5577324"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:06.050195Z",
     "start_time": "2025-11-23T06:24:04.822959Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentry_sdk.utils import epoch\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:06.055472Z",
     "start_time": "2025-11-23T06:24:06.053428Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1e248f0cccac7414",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:06.059988Z",
     "start_time": "2025-11-23T06:24:06.057627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Patch Embedding\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_dim, patch_size):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "         x = self.conv2d(x)\n",
    "         x = x.flatten(2)\n",
    "         return x.transpose(1, 2)"
   ],
   "id": "b094b84d00d0216c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:06.110756Z",
     "start_time": "2025-11-23T06:24:06.063980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DEIT(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, num_classes, embedding_dim, depth, ff_dim, dropout , n_head):\n",
    "        super(DEIT, self).__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(in_channels, embedding_dim, patch_size)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        #CLS TOKEN\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim) * 0.02)\n",
    "        self.dist_token = nn.Parameter(torch.randn(1, 1, embedding_dim) * 0.02)\n",
    "\n",
    "        #Postional Embedding\n",
    "        self.pos_embed  = nn.Parameter(torch.randn(1, num_patches + 2, embedding_dim) * 0.02)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #Transformer  encoder\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, n_head, ff_dim, dropout, activation= 'gelu', batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        #Two Head DEiT\n",
    "        self.head_cls = nn.Linear(embedding_dim, num_classes)\n",
    "        self.head_dist = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PATCH EMBEDDING\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        B = x.size(0)\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.dist_token.expand(B, -1, -1)\n",
    "        # CONCAT TOKENS\n",
    "        x = torch.cat([cls_token, dist_token, x], dim=1)\n",
    "        # POSITIONAL EMBEDDING\n",
    "        pos = self.pos_embed[:, :x.size(1), :]\n",
    "        x = x + pos\n",
    "        # DROPOUT\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # TRANSFORMER\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        cls_out = self.head_cls(x[:, 0])\n",
    "        dist_out = self.head_dist(x[:, 1])\n",
    "        return cls_out, dist_out\n"
   ],
   "id": "68b7d8b1e0619956",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:06.117302Z",
     "start_time": "2025-11-23T06:24:06.114893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def deit_loss(cls_out, dist_out, teacher_logits, labels, T=4.0, alpha=0.95):\n",
    "\n",
    "    ce_loss = F.cross_entropy(cls_out, labels)\n",
    "    student_log_probs = F.log_softmax(dist_out / T, dim=-1)\n",
    "    teacher_probs = F.softmax(teacher_logits.detach() / T, dim=-1)\n",
    "\n",
    "    dist_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (T * T)\n",
    "\n",
    "    return alpha * dist_loss + (1 - alpha) * ce_loss, ce_loss, dist_loss"
   ],
   "id": "203cd703c78c9ca8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:07.857979Z",
     "start_time": "2025-11-23T06:24:06.119803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_data  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=128)\n"
   ],
   "id": "a3e303da0460e9d4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:10.345127Z",
     "start_time": "2025-11-23T06:24:10.343266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #Define Teacher Model\n",
    "# import torchvision.models as models\n",
    "#\n",
    "# teacher = models.resnet50(weights=None)\n",
    "# teacher.fc = nn.Linear(2048, 10)\n",
    "# teacher = teacher.to(device)\n",
    "#\n",
    "# epochs = 10\n",
    "# optimizer = torch.optim.Adam(teacher.parameters(), lr = 3e-4)\n",
    "#\n",
    "# for epoch in range(epochs):\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = teacher(images)\n",
    "#         loss = F.cross_entropy(logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ],
   "id": "c998302ae14adb25",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:10.742707Z",
     "start_time": "2025-11-23T06:24:10.347284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.models as models\n",
    "teacher = models.resnet50(weights=None)\n",
    "teacher.fc = nn.Linear(2048, 10)\n",
    "\n",
    "teacher.load_state_dict(torch.load(\"teacher_cifar10.pth\"))\n",
    "teacher.eval()\n",
    "\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "teacher.to(device)\n"
   ],
   "id": "310c3b783b0aa752",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:10.754408Z",
     "start_time": "2025-11-23T06:24:10.747002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval(model, loader, t_model):\n",
    "    model.eval()\n",
    "    teacher.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_ce_loss = 0\n",
    "    total_dist_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        teacher_prob = torch.softmax(t_model(images), dim=-1)\n",
    "        teacher_prob = teacher_prob.to(device)\n",
    "        cls_out, dist_out  = model(images)\n",
    "        loss, ce_loss, dist_loss = deit_loss(cls_out, dist_out, teacher_prob, labels)\n",
    "        total_loss += loss.item()\n",
    "        total_ce_loss += ce_loss.item()\n",
    "        total_dist_loss += dist_loss.item()\n",
    "\n",
    "        preds = cls_out.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(loader)\n",
    "    average_ce_loss = total_ce_loss / len(loader)\n",
    "    average_dist_loss = total_dist_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f'Eval Loss = {average_loss}, CE = {average_ce_loss}, Dist = {average_dist_loss}, Accuracy = {accuracy}')\n",
    "    return average_loss, accuracy\n",
    "def training_func(model, t_model, optimizer, scheduler, train_loader, valid_loader, epochs):\n",
    "    history = {'train_loss': [], 'ce_loss': [], 'dist_loss': [], 'train_accuracy': [],'validation_accuracy': [] }\n",
    "    t_model.eval()\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_ce_loss = 0\n",
    "        total_dist_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for image, labels in train_loader:\n",
    "            images, labels = image.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                teacher_prob = t_model(images)\n",
    "                teacher_prob = teacher_prob.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            cls_out, dist_out = model(images)\n",
    "            loss, ce_loss, dist_loss = deit_loss(cls_out, dist_out, teacher_prob, labels, T = 4, alpha = 0.9)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.detach().item()\n",
    "            total_ce_loss += ce_loss.item()\n",
    "            total_dist_loss += dist_loss.item()\n",
    "\n",
    "            preds = cls_out.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_ce_loss = total_ce_loss / len(train_loader)\n",
    "        epoch_dist_loss = total_dist_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        _, val_accuracy = eval(model, valid_loader, t_model)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['ce_loss'].append(epoch_ce_loss)\n",
    "        history['dist_loss'].append(epoch_dist_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['validation_accuracy'].append(val_accuracy)\n",
    "\n",
    "\n",
    "        print(f'Epoch {e + 1}/ {epochs}, '\n",
    "                  f'Loss: {epoch_loss:.4f}, '\n",
    "                  f'Dist: {epoch_ce_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    return history\n"
   ],
   "id": "ef8af7a9db58ccd2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:10.940713Z",
     "start_time": "2025-11-23T06:24:10.759483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "student = DEIT(img_size= 32, patch_size= 4, num_classes= 10, embedding_dim= 192, depth= 4, n_head= 4, ff_dim= 384, in_channels=3 , dropout= 0.1).to(device)\n",
    "\n",
    "images = torch.randn(2,3,32,32).to(device)\n",
    "cls, dist = student(images)\n",
    "loss = (cls.mean() + dist.mean())\n",
    "loss.backward()\n",
    "\n",
    "print(student.head_cls.weight.grad is not None)\n",
    "print(student.head_dist.weight.grad is not None)\n"
   ],
   "id": "f4e04ce7294a7d19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:25:32.982508Z",
     "start_time": "2025-11-23T06:25:32.979467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "class WarmupCosineScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_epochs, max_epochs, warmup_start_lr=1e-6, eta_min=1e-5):\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.eta_min = eta_min\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "\n",
    "            return [self.warmup_start_lr + (base_lr - self.warmup_start_lr) * (self.last_epoch / self.warmup_epochs)\n",
    "                    for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            progress = (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "warmup_epochs = 5\n",
    "total_epochs = 50\n"
   ],
   "id": "b8e8db5280ad9d28",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-23T06:25:33.448594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    student.parameters(),\n",
    "    lr=5e-4,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.05\n",
    ")\n",
    "scheduler = WarmupCosineScheduler(\n",
    "    optimizer,\n",
    "    warmup_epochs=warmup_epochs,\n",
    "    max_epochs=total_epochs,\n",
    "    warmup_start_lr=1e-6,\n",
    "    eta_min=1e-5\n",
    ")\n",
    "training_func(student, teacher, optimizer, scheduler= scheduler, train_loader= train_loader, valid_loader= test_loader, epochs= total_epochs)\n",
    "eval(student, test_loader, teacher)"
   ],
   "id": "7ab9fc30869841a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss = 0.23189290867576115, CE = 2.3428510804719562, Dist = 0.12078984732492061, Accuracy = 0.0626\n",
      "Epoch 1/ 50, Loss: 4.9811, Dist: 2.3839, Train Accuracy: 0.0858, Validation Accuracy: 0.0626\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%%sql\n",
   "id": "9316ef9ce9ab3c6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "64bd11ae69a7d07b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Implementation of DEIT from scratch",
   "id": "bd6ea848f5577324"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-23T05:43:29.908017Z",
     "start_time": "2025-11-23T05:43:28.473810Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentry_sdk.utils import epoch\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:43:29.914482Z",
     "start_time": "2025-11-23T05:43:29.913062Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1e248f0cccac7414",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:43:29.920890Z",
     "start_time": "2025-11-23T05:43:29.917392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Patch Embedding\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_dim, patch_size):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "         x = self.conv2d(x)\n",
    "         x = x.flatten(2)\n",
    "         return x.transpose(1, 2)"
   ],
   "id": "b094b84d00d0216c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:43:29.928877Z",
     "start_time": "2025-11-23T05:43:29.923958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DEIT(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, num_classes, embedding_dim, depth, ff_dim, dropout , n_head):\n",
    "        super(DEIT, self).__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(in_channels, embedding_dim, patch_size)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        #CLS TOKEN\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim) * 0.02)\n",
    "        self.dist_token = nn.Parameter(torch.randn(1, 1, embedding_dim) * 0.02)\n",
    "\n",
    "        #Postional Embedding\n",
    "        self.pos_embed  = nn.Parameter(torch.randn(1, num_patches + 2, embedding_dim) * 0.02)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #Transformer  encoder\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, n_head, ff_dim, dropout, activation= 'gelu', batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        #Two Head DEiT\n",
    "        self.head_cls = nn.Linear(embedding_dim, num_classes)\n",
    "        self.head_dist = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"\\n---- FORWARD DEBUG ----\")\n",
    "        print(\"input:\", x.shape)  # <-- print batch first\n",
    "\n",
    "        # PATCH EMBEDDING\n",
    "        x = self.patch_embed(x)\n",
    "        print(\"after patch_embed:\", x.shape)\n",
    "\n",
    "        B = x.size(0)\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.dist_token.expand(B, -1, -1)\n",
    "        print(\"cls_token:\", cls_token.shape)\n",
    "        print(\"dist_token:\", dist_token.shape)\n",
    "\n",
    "        # CONCAT TOKENS\n",
    "        x = torch.cat([cls_token, dist_token, x], dim=1)\n",
    "        print(\"after concat:\", x.shape)\n",
    "\n",
    "        # POSITIONAL EMBEDDING\n",
    "        pos = self.pos_embed[:, :x.size(1), :]\n",
    "        print(\"pos_embed:\", pos.shape)\n",
    "\n",
    "        x = x + pos\n",
    "        print(\"after +pos:\", x.shape)\n",
    "\n",
    "        # DROPOUT\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # TRANSFORMER\n",
    "        x = self.transformer(x)\n",
    "        print(\"after transformer:\", x.shape)\n",
    "\n",
    "        # LAYER NORM\n",
    "        x = self.norm(x)\n",
    "        print(\"after norm:\", x.shape)\n",
    "\n",
    "        cls_out = self.head_cls(x[:, 0])\n",
    "        dist_out = self.head_dist(x[:, 1])\n",
    "\n",
    "        print(\"cls_out:\", cls_out.shape)\n",
    "        print(\"dist_out:\", dist_out.shape)\n",
    "        print(\"-----------------------\\n\")\n",
    "\n",
    "        return cls_out, dist_out\n"
   ],
   "id": "68b7d8b1e0619956",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:43:29.934734Z",
     "start_time": "2025-11-23T05:43:29.932399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Function to calculate total loss\n",
    "def deit_loss(cls_out, dist_out, teacher_probs, labels, T = 1.0):\n",
    "    ce_loss = F.cross_entropy(cls_out, labels)\n",
    "\n",
    "    #Distillation\n",
    "    k1_loss = torch.nn.functional.kl_div(\n",
    "        F.log_softmax(dist_out/ T, dim=-1),\n",
    "        teacher_probs,\n",
    "        reduction ='batchmean'\n",
    "    ) * (T * T)\n",
    "    return ce_loss + k1_loss, ce_loss, k1_loss"
   ],
   "id": "203cd703c78c9ca8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:43:31.884221Z",
     "start_time": "2025-11-23T05:43:29.939416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_data  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=128)\n"
   ],
   "id": "a3e303da0460e9d4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:43:38.448184Z",
     "start_time": "2025-11-23T05:43:38.445473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #Define Teacher Model\n",
    "# import torchvision.models as models\n",
    "#\n",
    "# teacher = models.resnet50(weights=None)\n",
    "# teacher.fc = nn.Linear(2048, 10)\n",
    "# teacher = teacher.to(device)\n",
    "#\n",
    "# epochs = 10\n",
    "# optimizer = torch.optim.Adam(teacher.parameters(), lr = 3e-4)\n",
    "#\n",
    "# for epoch in range(epochs):\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = teacher(images)\n",
    "#         loss = F.cross_entropy(logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ],
   "id": "c998302ae14adb25",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:43:39.035559Z",
     "start_time": "2025-11-23T05:43:38.451942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.models as model\n",
    "teacher = model.resnet50(weights=None)\n",
    "teacher.fc = nn.Linear(2048, 10)\n",
    "\n",
    "teacher.load_state_dict(torch.load(\"teacher_cifar10.pth\"))\n",
    "teacher.eval()\n",
    "\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "teacher.to(device)\n"
   ],
   "id": "310c3b783b0aa752",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:43:39.045737Z",
     "start_time": "2025-11-23T05:43:39.040910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval(model, loader, t_model):\n",
    "    model.eval()\n",
    "    teacher.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_ce_loss = 0\n",
    "    total_dist_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        teacher_prob = torch.softmax(t_model(images), dim=-1)\n",
    "        cls_out, dist_out  = model(images)\n",
    "        loss, ce_loss, dist_loss = deit_loss(cls_out, dist_out, teacher_prob, labels)\n",
    "        total_loss += loss.item()\n",
    "        total_ce_loss += ce_loss.item()\n",
    "        total_dist_loss += dist_loss.item()\n",
    "\n",
    "        preds = cls_out.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(loader)\n",
    "    average_ce_loss = total_ce_loss / len(loader)\n",
    "    average_dist_loss = total_dist_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f'Eval Loss = {average_loss}, CE = {average_ce_loss}, Dist = {average_dist_loss}, Accuracy = {accuracy}')\n",
    "    return average_loss, accuracy\n",
    "def training_func(model, t_model, optimizer, scheduler, train_loader, valid_loader, epochs):\n",
    "    history = {'train_loss': [], 'ce_loss': [], 'dist_loss': [], 'train_accuracy': [], 'validation_accuracy': [] }\n",
    "    t_model.eval()\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_ce_loss = 0\n",
    "        total_dist_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for image, labels in train_loader:\n",
    "            images, labels = image.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                teacher_prob = torch.softmax(t_model(images), dim=-1)\n",
    "                optimizer.zero_grad()\n",
    "                cls_out, dist_out = model(images)\n",
    "                loss, ce_loss, dist_loss = deit_loss(cls_out, dist_out, teacher_prob, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_ce_loss += ce_loss.item()\n",
    "                total_dist_loss += dist_loss.item()\n",
    "\n",
    "                preds = cls_out.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            epoch_loss = total_loss / len(train_loader)\n",
    "            epoch_ce_loss = total_ce_loss / len(train_loader)\n",
    "            epoch_dist_loss = total_dist_loss / len(train_loader)\n",
    "            train_accuracy = correct / total\n",
    "\n",
    "            _, val_accuracy = eval(model, valid_loader, t_model)\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            history['train_loss'].append(epoch_loss)\n",
    "            history['train_ce_loss'].append(epoch_ce_loss)\n",
    "            history['train_dist_loss'].append(epoch_dist_loss)\n",
    "            history['train_accuracy'].append(train_accuracy)\n",
    "            history['validation_loss'].append(epoch_loss)\n",
    "\n",
    "\n",
    "            print(f'Epoch {e + 1}/ {epochs}, '\n",
    "                  f'Loss: {epoch_loss:.4f}, '\n",
    "                  f'Dist: {epoch_ce_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    return history\n"
   ],
   "id": "ef8af7a9db58ccd2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:43:39.271348Z",
     "start_time": "2025-11-23T05:43:39.073660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "student = DEIT(img_size= 32, patch_size= 4, num_classes= 10, embedding_dim= 192, depth= 4, n_head= 3, ff_dim= 384, in_channels=3 , dropout= 0.1).to(device)\n",
    "\n",
    "images = torch.randn(2,3,32,32).to(device)\n",
    "cls, dist = student(images)\n",
    "loss = (cls.mean() + dist.mean())\n",
    "loss.backward()\n",
    "\n",
    "print(student.head_cls.weight.grad is not None)\n",
    "print(student.head_dist.weight.grad is not None)\n"
   ],
   "id": "f4e04ce7294a7d19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- FORWARD DEBUG ----\n",
      "input: torch.Size([2, 3, 32, 32])\n",
      "after patch_embed: torch.Size([2, 64, 192])\n",
      "cls_token: torch.Size([2, 1, 192])\n",
      "dist_token: torch.Size([2, 1, 192])\n",
      "after concat: torch.Size([2, 66, 192])\n",
      "pos_embed: torch.Size([1, 66, 192])\n",
      "after +pos: torch.Size([2, 66, 192])\n",
      "after transformer: torch.Size([2, 66, 192])\n",
      "after norm: torch.Size([2, 66, 192])\n",
      "cls_out: torch.Size([2, 10])\n",
      "dist_out: torch.Size([2, 10])\n",
      "-----------------------\n",
      "\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\modam\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:46:56.919230Z",
     "start_time": "2025-11-23T05:46:56.905426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "cls_out, dist_out = student(images)\n"
   ],
   "id": "7a126e4934375e54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- FORWARD DEBUG ----\n",
      "input: torch.Size([64, 3, 32, 32])\n",
      "after patch_embed: torch.Size([64, 64, 192])\n",
      "cls_token: torch.Size([64, 1, 192])\n",
      "dist_token: torch.Size([64, 1, 192])\n",
      "after concat: torch.Size([64, 66, 192])\n",
      "pos_embed: torch.Size([1, 66, 192])\n",
      "after +pos: torch.Size([64, 66, 192])\n",
      "after transformer: torch.Size([64, 66, 192])\n",
      "after norm: torch.Size([64, 66, 192])\n",
      "cls_out: torch.Size([64, 10])\n",
      "dist_out: torch.Size([64, 10])\n",
      "-----------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:46:57.852499Z",
     "start_time": "2025-11-23T05:46:57.736148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cls_out, dist_out = student(images)\n",
    "\n",
    "print(\"cls_out:\", cls_out.requires_grad)\n",
    "print(\"dist_out:\", dist_out.requires_grad)\n",
    "\n",
    "teacher_prob = torch.softmax(teacher(images), dim=-1)\n",
    "loss, ce_loss, dist_loss = deit_loss(cls_out, dist_out, teacher_prob, labels)\n",
    "\n",
    "print(\"loss:\", loss.requires_grad)\n",
    "print(\"ce_loss:\", ce_loss.requires_grad)\n",
    "print(\"dist_loss:\", dist_loss.requires_grad)\n"
   ],
   "id": "3d262de5d9bf9f23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- FORWARD DEBUG ----\n",
      "input: torch.Size([64, 3, 32, 32])\n",
      "after patch_embed: torch.Size([64, 64, 192])\n",
      "cls_token: torch.Size([64, 1, 192])\n",
      "dist_token: torch.Size([64, 1, 192])\n",
      "after concat: torch.Size([64, 66, 192])\n",
      "pos_embed: torch.Size([1, 66, 192])\n",
      "after +pos: torch.Size([64, 66, 192])\n",
      "after transformer: torch.Size([64, 66, 192])\n",
      "after norm: torch.Size([64, 66, 192])\n",
      "cls_out: torch.Size([64, 10])\n",
      "dist_out: torch.Size([64, 10])\n",
      "-----------------------\n",
      "\n",
      "cls_out: True\n",
      "dist_out: True\n",
      "loss: True\n",
      "ce_loss: True\n",
      "dist_loss: True\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T05:47:04.020655Z",
     "start_time": "2025-11-23T05:47:03.890554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr= 3e-4)\n",
    "training_func(student, teacher, optimizer, scheduler= None, train_loader= train_loader, valid_loader= test_loader, epochs= 10)\n",
    "eval(student, test_loader, teacher)"
   ],
   "id": "7ab9fc30869841a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- FORWARD DEBUG ----\n",
      "input: torch.Size([64, 3, 32, 32])\n",
      "after patch_embed: torch.Size([64, 64, 192])\n",
      "cls_token: torch.Size([64, 1, 192])\n",
      "dist_token: torch.Size([64, 1, 192])\n",
      "after concat: torch.Size([64, 66, 192])\n",
      "pos_embed: torch.Size([1, 66, 192])\n",
      "after +pos: torch.Size([64, 66, 192])\n",
      "after transformer: torch.Size([64, 66, 192])\n",
      "after norm: torch.Size([64, 66, 192])\n",
      "cls_out: torch.Size([64, 10])\n",
      "dist_out: torch.Size([64, 10])\n",
      "-----------------------\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m optimizer = torch.optim.AdamW(student.parameters(), lr= \u001B[32m3e-4\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mtraining_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mteacher\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[38;5;28meval\u001B[39m(student, test_loader, teacher)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 48\u001B[39m, in \u001B[36mtraining_func\u001B[39m\u001B[34m(model, t_model, optimizer, scheduler, train_loader, valid_loader, epochs)\u001B[39m\n\u001B[32m     46\u001B[39m cls_out, dist_out = model(images)\n\u001B[32m     47\u001B[39m loss, ce_loss, dist_loss = deit_loss(cls_out, dist_out, teacher_prob, labels)\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     49\u001B[39m optimizer.step()\n\u001B[32m     51\u001B[39m total_loss += loss.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mRuntimeError\u001B[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "64bd11ae69a7d07b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

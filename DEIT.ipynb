{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Implementation of DEIT from scratch",
   "id": "bd6ea848f5577324"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:06.050195Z",
     "start_time": "2025-11-23T06:24:04.822959Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentry_sdk.utils import epoch\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:06.055472Z",
     "start_time": "2025-11-23T06:24:06.053428Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1e248f0cccac7414",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:06.059988Z",
     "start_time": "2025-11-23T06:24:06.057627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Patch Embedding\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_dim, patch_size):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "         x = self.conv2d(x)\n",
    "         x = x.flatten(2)\n",
    "         return x.transpose(1, 2)"
   ],
   "id": "b094b84d00d0216c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:06.110756Z",
     "start_time": "2025-11-23T06:24:06.063980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DEIT(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, num_classes, embedding_dim, depth, ff_dim, dropout , n_head):\n",
    "        super(DEIT, self).__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(in_channels, embedding_dim, patch_size)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        #CLS TOKEN\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim) * 0.02)\n",
    "        self.dist_token = nn.Parameter(torch.randn(1, 1, embedding_dim) * 0.02)\n",
    "\n",
    "        #Postional Embedding\n",
    "        self.pos_embed  = nn.Parameter(torch.randn(1, num_patches + 2, embedding_dim) * 0.02)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #Transformer  encoder\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, n_head, ff_dim, dropout, activation= 'gelu', batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        #Two Head DEiT\n",
    "        self.head_cls = nn.Linear(embedding_dim, num_classes)\n",
    "        self.head_dist = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PATCH EMBEDDING\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        B = x.size(0)\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.dist_token.expand(B, -1, -1)\n",
    "        # CONCAT TOKENS\n",
    "        x = torch.cat([cls_token, dist_token, x], dim=1)\n",
    "        # POSITIONAL EMBEDDING\n",
    "        pos = self.pos_embed[:, :x.size(1), :]\n",
    "        x = x + pos\n",
    "        # DROPOUT\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # TRANSFORMER\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        cls_out = self.head_cls(x[:, 0])\n",
    "        dist_out = self.head_dist(x[:, 1])\n",
    "        return cls_out, dist_out\n"
   ],
   "id": "68b7d8b1e0619956",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:06.117302Z",
     "start_time": "2025-11-23T06:24:06.114893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def deit_loss(cls_out, dist_out, teacher_logits, labels, T=4.0, alpha=0.95):\n",
    "\n",
    "    ce_loss = F.cross_entropy(cls_out, labels)\n",
    "    student_log_probs = F.log_softmax(dist_out / T, dim=-1)\n",
    "    teacher_probs = F.softmax(teacher_logits.detach() / T, dim=-1)\n",
    "\n",
    "    dist_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (T * T)\n",
    "\n",
    "    return alpha * dist_loss + (1 - alpha) * ce_loss, ce_loss, dist_loss"
   ],
   "id": "203cd703c78c9ca8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:07.857979Z",
     "start_time": "2025-11-23T06:24:06.119803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_data  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=128)\n"
   ],
   "id": "a3e303da0460e9d4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:10.345127Z",
     "start_time": "2025-11-23T06:24:10.343266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #Define Teacher Model\n",
    "# import torchvision.models as models\n",
    "#\n",
    "# teacher = models.resnet50(weights=None)\n",
    "# teacher.fc = nn.Linear(2048, 10)\n",
    "# teacher = teacher.to(device)\n",
    "#\n",
    "# epochs = 10\n",
    "# optimizer = torch.optim.Adam(teacher.parameters(), lr = 3e-4)\n",
    "#\n",
    "# for epoch in range(epochs):\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = teacher(images)\n",
    "#         loss = F.cross_entropy(logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ],
   "id": "c998302ae14adb25",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:10.742707Z",
     "start_time": "2025-11-23T06:24:10.347284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.models as models\n",
    "teacher = models.resnet50(weights=None)\n",
    "teacher.fc = nn.Linear(2048, 10)\n",
    "\n",
    "teacher.load_state_dict(torch.load(\"teacher_cifar10.pth\"))\n",
    "teacher.eval()\n",
    "\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "teacher.to(device)\n"
   ],
   "id": "310c3b783b0aa752",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:10.754408Z",
     "start_time": "2025-11-23T06:24:10.747002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval(model, loader, t_model):\n",
    "    model.eval()\n",
    "    teacher.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_ce_loss = 0\n",
    "    total_dist_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        teacher_prob = torch.softmax(t_model(images), dim=-1)\n",
    "        teacher_prob = teacher_prob.to(device)\n",
    "        cls_out, dist_out  = model(images)\n",
    "        loss, ce_loss, dist_loss = deit_loss(cls_out, dist_out, teacher_prob, labels)\n",
    "        total_loss += loss.item()\n",
    "        total_ce_loss += ce_loss.item()\n",
    "        total_dist_loss += dist_loss.item()\n",
    "\n",
    "        preds = cls_out.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(loader)\n",
    "    average_ce_loss = total_ce_loss / len(loader)\n",
    "    average_dist_loss = total_dist_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f'Eval Loss = {average_loss}, CE = {average_ce_loss}, Dist = {average_dist_loss}, Accuracy = {accuracy}')\n",
    "    return average_loss, accuracy\n",
    "def training_func(model, t_model, optimizer, scheduler, train_loader, valid_loader, epochs):\n",
    "    history = {'train_loss': [], 'ce_loss': [], 'dist_loss': [], 'train_accuracy': [],'validation_accuracy': [] }\n",
    "    t_model.eval()\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_ce_loss = 0\n",
    "        total_dist_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for image, labels in train_loader:\n",
    "            images, labels = image.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                teacher_prob = t_model(images)\n",
    "                teacher_prob = teacher_prob.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            cls_out, dist_out = model(images)\n",
    "            loss, ce_loss, dist_loss = deit_loss(cls_out, dist_out, teacher_prob, labels, T = 4, alpha = 0.9)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.detach().item()\n",
    "            total_ce_loss += ce_loss.item()\n",
    "            total_dist_loss += dist_loss.item()\n",
    "\n",
    "            preds = cls_out.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_ce_loss = total_ce_loss / len(train_loader)\n",
    "        epoch_dist_loss = total_dist_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        _, val_accuracy = eval(model, valid_loader, t_model)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['ce_loss'].append(epoch_ce_loss)\n",
    "        history['dist_loss'].append(epoch_dist_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['validation_accuracy'].append(val_accuracy)\n",
    "\n",
    "\n",
    "        print(f'Epoch {e + 1}/ {epochs}, '\n",
    "                  f'Loss: {epoch_loss:.4f}, '\n",
    "                  f'Dist: {epoch_ce_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    return history\n"
   ],
   "id": "ef8af7a9db58ccd2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:24:10.940713Z",
     "start_time": "2025-11-23T06:24:10.759483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "student = DEIT(img_size= 32, patch_size= 4, num_classes= 10, embedding_dim= 192, depth= 4, n_head= 4, ff_dim= 384, in_channels=3 , dropout= 0.1).to(device)\n",
    "\n",
    "images = torch.randn(2,3,32,32).to(device)\n",
    "cls, dist = student(images)\n",
    "loss = (cls.mean() + dist.mean())\n",
    "loss.backward()\n",
    "\n",
    "print(student.head_cls.weight.grad is not None)\n",
    "print(student.head_dist.weight.grad is not None)\n"
   ],
   "id": "f4e04ce7294a7d19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:25:32.982508Z",
     "start_time": "2025-11-23T06:25:32.979467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "class WarmupCosineScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_epochs, max_epochs, warmup_start_lr=1e-6, eta_min=1e-5):\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.eta_min = eta_min\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "\n",
    "            return [self.warmup_start_lr + (base_lr - self.warmup_start_lr) * (self.last_epoch / self.warmup_epochs)\n",
    "                    for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            progress = (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "warmup_epochs = 5\n",
    "total_epochs = 50\n"
   ],
   "id": "b8e8db5280ad9d28",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:36:02.628173Z",
     "start_time": "2025-11-23T06:25:33.448594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    student.parameters(),\n",
    "    lr=5e-4,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.05\n",
    ")\n",
    "scheduler = WarmupCosineScheduler(\n",
    "    optimizer,\n",
    "    warmup_epochs=warmup_epochs,\n",
    "    max_epochs=total_epochs,\n",
    "    warmup_start_lr=1e-6,\n",
    "    eta_min=1e-5\n",
    ")\n",
    "training_func(student, teacher, optimizer, scheduler= scheduler, train_loader= train_loader, valid_loader= test_loader, epochs= total_epochs)\n",
    "eval(student, test_loader, teacher)"
   ],
   "id": "7ab9fc30869841a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss = 0.23189290867576115, CE = 2.3428510804719562, Dist = 0.12078984732492061, Accuracy = 0.0626\n",
      "Epoch 1/ 50, Loss: 4.9811, Dist: 2.3839, Train Accuracy: 0.0858, Validation Accuracy: 0.0626\n",
      "Eval Loss = 2.882244399831265, CE = 1.5561893725696998, Dist = 2.9520367851740197, Accuracy = 0.4272\n",
      "Epoch 2/ 50, Loss: 3.0129, Dist: 1.8148, Train Accuracy: 0.3150, Validation Accuracy: 0.4272\n",
      "Eval Loss = 3.7102090407021437, CE = 1.2807800392561322, Dist = 3.838073769702187, Accuracy = 0.5304\n",
      "Epoch 3/ 50, Loss: 1.9506, Dist: 1.4309, Train Accuracy: 0.4720, Validation Accuracy: 0.5304\n",
      "Eval Loss = 3.6354277918610394, CE = 1.1834615484068665, Dist = 3.764478662345983, Accuracy = 0.5653\n",
      "Epoch 4/ 50, Loss: 1.5480, Dist: 1.2610, Train Accuracy: 0.5407, Validation Accuracy: 0.5653\n",
      "Eval Loss = 3.85116297987443, CE = 1.1155788770204857, Dist = 3.995141144040265, Accuracy = 0.5933\n",
      "Epoch 5/ 50, Loss: 1.3497, Dist: 1.1652, Train Accuracy: 0.5785, Validation Accuracy: 0.5933\n",
      "Eval Loss = 3.966380565981322, CE = 1.0813481611541556, Dist = 4.118224433705777, Accuracy = 0.6103\n",
      "Epoch 6/ 50, Loss: 1.2210, Dist: 1.0964, Train Accuracy: 0.6057, Validation Accuracy: 0.6103\n",
      "Eval Loss = 4.054561524451533, CE = 1.0571118593215942, Dist = 4.212322081191631, Accuracy = 0.6134\n",
      "Epoch 7/ 50, Loss: 1.1138, Dist: 1.0327, Train Accuracy: 0.6309, Validation Accuracy: 0.6134\n",
      "Eval Loss = 4.127357718310779, CE = 0.9709498678581624, Dist = 4.293484488620034, Accuracy = 0.6532\n",
      "Epoch 8/ 50, Loss: 1.0195, Dist: 0.9763, Train Accuracy: 0.6496, Validation Accuracy: 0.6532\n",
      "Eval Loss = 4.180104608777203, CE = 0.9181286255015603, Dist = 4.351787591282325, Accuracy = 0.6749\n",
      "Epoch 9/ 50, Loss: 0.9365, Dist: 0.9322, Train Accuracy: 0.6682, Validation Accuracy: 0.6749\n",
      "Eval Loss = 4.022973996174486, CE = 0.9855120000959952, Dist = 4.18284050120583, Accuracy = 0.6527\n",
      "Epoch 10/ 50, Loss: 0.8772, Dist: 0.8905, Train Accuracy: 0.6816, Validation Accuracy: 0.6527\n",
      "Eval Loss = 4.369503956806811, CE = 0.9234225395359571, Dist = 4.550876729096038, Accuracy = 0.6733\n",
      "Epoch 11/ 50, Loss: 0.8336, Dist: 0.8585, Train Accuracy: 0.6947, Validation Accuracy: 0.6733\n",
      "Eval Loss = 4.35850476313241, CE = 0.8933725764479818, Dist = 4.54088023040868, Accuracy = 0.6906\n",
      "Epoch 12/ 50, Loss: 0.7787, Dist: 0.8200, Train Accuracy: 0.7073, Validation Accuracy: 0.6906\n",
      "Eval Loss = 4.71519395973109, CE = 0.8504822563521469, Dist = 4.918599907355972, Accuracy = 0.7058\n",
      "Epoch 13/ 50, Loss: 0.7426, Dist: 0.7996, Train Accuracy: 0.7165, Validation Accuracy: 0.7058\n",
      "Eval Loss = 4.549049311046359, CE = 0.8411762246602699, Dist = 4.744200621979146, Accuracy = 0.7057\n",
      "Epoch 14/ 50, Loss: 0.7203, Dist: 0.7779, Train Accuracy: 0.7236, Validation Accuracy: 0.7057\n",
      "Eval Loss = 4.346112882034688, CE = 0.8004988650732403, Dist = 4.532724217523502, Accuracy = 0.7187\n",
      "Epoch 15/ 50, Loss: 0.6827, Dist: 0.7493, Train Accuracy: 0.7333, Validation Accuracy: 0.7187\n",
      "Eval Loss = 4.447775985621199, CE = 0.8250360617154762, Dist = 4.638446584532533, Accuracy = 0.7094\n",
      "Epoch 16/ 50, Loss: 0.6600, Dist: 0.7353, Train Accuracy: 0.7379, Validation Accuracy: 0.7094\n",
      "Eval Loss = 4.673096173926245, CE = 0.8046887788591506, Dist = 4.876696665075761, Accuracy = 0.7208\n",
      "Epoch 17/ 50, Loss: 0.6367, Dist: 0.7153, Train Accuracy: 0.7449, Validation Accuracy: 0.7208\n",
      "Eval Loss = 4.731588019600397, CE = 0.7757068600835679, Dist = 4.939792367476452, Accuracy = 0.7314\n",
      "Epoch 18/ 50, Loss: 0.6175, Dist: 0.7023, Train Accuracy: 0.7499, Validation Accuracy: 0.7314\n",
      "Eval Loss = 4.425084735773787, CE = 0.7893390202824073, Dist = 4.616439825371851, Accuracy = 0.727\n",
      "Epoch 19/ 50, Loss: 0.6008, Dist: 0.6885, Train Accuracy: 0.7548, Validation Accuracy: 0.7270\n",
      "Eval Loss = 4.2681662010241155, CE = 0.8049144254455084, Dist = 4.450442673284797, Accuracy = 0.7212\n",
      "Epoch 20/ 50, Loss: 0.5758, Dist: 0.6724, Train Accuracy: 0.7594, Validation Accuracy: 0.7212\n",
      "Eval Loss = 4.561656662180454, CE = 0.7678368680084808, Dist = 4.761331443545185, Accuracy = 0.7368\n",
      "Epoch 21/ 50, Loss: 0.5590, Dist: 0.6569, Train Accuracy: 0.7667, Validation Accuracy: 0.7368\n",
      "Eval Loss = 4.653368780884562, CE = 0.767005932481983, Dist = 4.857914296886589, Accuracy = 0.7357\n",
      "Epoch 22/ 50, Loss: 0.5412, Dist: 0.6469, Train Accuracy: 0.7698, Validation Accuracy: 0.7357\n",
      "Eval Loss = 4.545642916160293, CE = 0.7506768077234679, Dist = 4.745378071748758, Accuracy = 0.7403\n",
      "Epoch 23/ 50, Loss: 0.5272, Dist: 0.6354, Train Accuracy: 0.7738, Validation Accuracy: 0.7403\n",
      "Eval Loss = 4.541922647741776, CE = 0.7446117650104475, Dist = 4.7417811985257305, Accuracy = 0.7464\n",
      "Epoch 24/ 50, Loss: 0.5121, Dist: 0.6220, Train Accuracy: 0.7802, Validation Accuracy: 0.7464\n",
      "Eval Loss = 4.514727993856503, CE = 0.7617555743531336, Dist = 4.7122529126420805, Accuracy = 0.7382\n",
      "Epoch 25/ 50, Loss: 0.4951, Dist: 0.6088, Train Accuracy: 0.7838, Validation Accuracy: 0.7382\n",
      "Eval Loss = 4.549270669116249, CE = 0.7389284874065013, Dist = 4.749815035469925, Accuracy = 0.7485\n",
      "Epoch 26/ 50, Loss: 0.4832, Dist: 0.5967, Train Accuracy: 0.7891, Validation Accuracy: 0.7485\n",
      "Eval Loss = 4.739952998825267, CE = 0.750843624902677, Dist = 4.9499062224279475, Accuracy = 0.7463\n",
      "Epoch 27/ 50, Loss: 0.4690, Dist: 0.5825, Train Accuracy: 0.7955, Validation Accuracy: 0.7463\n",
      "Eval Loss = 4.5228759487972985, CE = 0.7290319027025488, Dist = 4.722552076170716, Accuracy = 0.753\n",
      "Epoch 28/ 50, Loss: 0.4549, Dist: 0.5707, Train Accuracy: 0.7967, Validation Accuracy: 0.7530\n",
      "Eval Loss = 4.698544007313402, CE = 0.7341601154472255, Dist = 4.907195851772646, Accuracy = 0.7515\n",
      "Epoch 29/ 50, Loss: 0.4444, Dist: 0.5623, Train Accuracy: 0.8013, Validation Accuracy: 0.7515\n",
      "Eval Loss = 4.602928327608712, CE = 0.7259271167501619, Dist = 4.80698105655139, Accuracy = 0.75\n",
      "Epoch 30/ 50, Loss: 0.4300, Dist: 0.5520, Train Accuracy: 0.8045, Validation Accuracy: 0.7500\n",
      "Eval Loss = 4.691142332704762, CE = 0.7173588849321196, Dist = 4.9002889379670345, Accuracy = 0.7577\n",
      "Epoch 31/ 50, Loss: 0.4211, Dist: 0.5464, Train Accuracy: 0.8062, Validation Accuracy: 0.7577\n",
      "Eval Loss = 4.78918876527231, CE = 0.712361749214462, Dist = 5.003758714168886, Accuracy = 0.7601\n",
      "Epoch 32/ 50, Loss: 0.4074, Dist: 0.5300, Train Accuracy: 0.8132, Validation Accuracy: 0.7601\n",
      "Eval Loss = 4.701084662087356, CE = 0.710835861254342, Dist = 4.911097864561443, Accuracy = 0.7648\n",
      "Epoch 33/ 50, Loss: 0.4001, Dist: 0.5235, Train Accuracy: 0.8162, Validation Accuracy: 0.7648\n",
      "Eval Loss = 4.68548513665984, CE = 0.7213949515849729, Dist = 4.894121538234662, Accuracy = 0.7601\n",
      "Epoch 34/ 50, Loss: 0.3922, Dist: 0.5186, Train Accuracy: 0.8168, Validation Accuracy: 0.7601\n",
      "Eval Loss = 4.707602887213985, CE = 0.7015219149710257, Dist = 4.918449299244941, Accuracy = 0.7664\n",
      "Epoch 35/ 50, Loss: 0.3786, Dist: 0.5077, Train Accuracy: 0.8207, Validation Accuracy: 0.7664\n",
      "Eval Loss = 4.726159970971603, CE = 0.7079972847353054, Dist = 4.937642296658287, Accuracy = 0.7629\n",
      "Epoch 36/ 50, Loss: 0.3717, Dist: 0.5032, Train Accuracy: 0.8223, Validation Accuracy: 0.7629\n",
      "Eval Loss = 4.706885657732999, CE = 0.7128109135959722, Dist = 4.91710021224203, Accuracy = 0.7643\n",
      "Epoch 37/ 50, Loss: 0.3650, Dist: 0.4936, Train Accuracy: 0.8240, Validation Accuracy: 0.7643\n",
      "Eval Loss = 4.830483961708938, CE = 0.715283037740973, Dist = 5.047073539299301, Accuracy = 0.7672\n",
      "Epoch 38/ 50, Loss: 0.3577, Dist: 0.4857, Train Accuracy: 0.8284, Validation Accuracy: 0.7672\n",
      "Eval Loss = 4.74080040485044, CE = 0.7085936620265623, Dist = 4.95302188245556, Accuracy = 0.7655\n",
      "Epoch 39/ 50, Loss: 0.3508, Dist: 0.4781, Train Accuracy: 0.8318, Validation Accuracy: 0.7655\n",
      "Eval Loss = 4.765044550352458, CE = 0.7053305521796022, Dist = 4.978713784036757, Accuracy = 0.7674\n",
      "Epoch 40/ 50, Loss: 0.3446, Dist: 0.4736, Train Accuracy: 0.8339, Validation Accuracy: 0.7674\n",
      "Eval Loss = 4.804904986031448, CE = 0.7085861129851281, Dist = 5.020500834984116, Accuracy = 0.7663\n",
      "Epoch 41/ 50, Loss: 0.3394, Dist: 0.4678, Train Accuracy: 0.8364, Validation Accuracy: 0.7663\n",
      "Eval Loss = 4.803872428362882, CE = 0.7091753833656069, Dist = 5.019382851033271, Accuracy = 0.7674\n",
      "Epoch 42/ 50, Loss: 0.3334, Dist: 0.4637, Train Accuracy: 0.8364, Validation Accuracy: 0.7674\n",
      "Eval Loss = 4.7110004575946665, CE = 0.7026005413713334, Dist = 4.921968936920166, Accuracy = 0.7682\n",
      "Epoch 43/ 50, Loss: 0.3293, Dist: 0.4619, Train Accuracy: 0.8363, Validation Accuracy: 0.7682\n",
      "Eval Loss = 4.765305639822272, CE = 0.7046453752849675, Dist = 4.979024699971646, Accuracy = 0.769\n",
      "Epoch 44/ 50, Loss: 0.3254, Dist: 0.4565, Train Accuracy: 0.8392, Validation Accuracy: 0.7690\n",
      "Eval Loss = 4.725887461553646, CE = 0.7030110151707372, Dist = 4.93761783913721, Accuracy = 0.7695\n",
      "Epoch 45/ 50, Loss: 0.3213, Dist: 0.4526, Train Accuracy: 0.8420, Validation Accuracy: 0.7695\n",
      "Eval Loss = 4.7768867589250394, CE = 0.7039506657968594, Dist = 4.991251903244212, Accuracy = 0.7666\n",
      "Epoch 46/ 50, Loss: 0.3185, Dist: 0.4496, Train Accuracy: 0.8404, Validation Accuracy: 0.7666\n",
      "Eval Loss = 4.713438845887969, CE = 0.7009941252726543, Dist = 4.924620205842996, Accuracy = 0.7708\n",
      "Epoch 47/ 50, Loss: 0.3162, Dist: 0.4442, Train Accuracy: 0.8442, Validation Accuracy: 0.7708\n",
      "Eval Loss = 4.726578317111051, CE = 0.7002731742738169, Dist = 4.938489183594909, Accuracy = 0.7698\n",
      "Epoch 48/ 50, Loss: 0.3141, Dist: 0.4472, Train Accuracy: 0.8418, Validation Accuracy: 0.7698\n",
      "Eval Loss = 4.769179229494892, CE = 0.7036920936801766, Dist = 4.983152280879926, Accuracy = 0.7684\n",
      "Epoch 49/ 50, Loss: 0.3145, Dist: 0.4471, Train Accuracy: 0.8424, Validation Accuracy: 0.7684\n",
      "Eval Loss = 4.754745791230021, CE = 0.7016040094291107, Dist = 4.968069082573999, Accuracy = 0.7676\n",
      "Epoch 50/ 50, Loss: 0.3125, Dist: 0.4466, Train Accuracy: 0.8422, Validation Accuracy: 0.7676\n",
      "Eval Loss = 4.754745791230021, CE = 0.7016040094291107, Dist = 4.968069082573999, Accuracy = 0.7676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.754745791230021, 0.7676)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### How to improve the current 76% accuracy\n",
    "\n",
    "To match or beat the original DeIT-Tiny Google paper:\n",
    "\n",
    "1. Zero-init cls_token, trunc_normal pos_embed\n",
    "2. Add LayerScale (1e-6) to each block\n",
    "3. Add stochastic depth (linear drop from 0.0 → 0.1–0.2)\n",
    "4. Train 200–300 epochs with cosine scheduler + warm restarts\n",
    "5. Use weight_decay=0.05 and AdamW\n",
    "6. Switch to relative position bias or DeiT-III improvements\n"
   ],
   "id": "3f9f34a32ecb601"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "64bd11ae69a7d07b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

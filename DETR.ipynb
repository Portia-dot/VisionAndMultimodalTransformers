{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:44.554980Z",
     "start_time": "2025-11-18T23:49:36.218711Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from comet_ml.logging_extensions.rich_decoration.environment import height\n",
    "from torchvision.ops import generalized_box_iou\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from typing import Tuple\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import random\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:44.571990Z",
     "start_time": "2025-11-18T23:49:44.567430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cxcywh_to_xyxy(boxes):\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    return torch.stack((x1, y1, x2, y2), dim=-1)"
   ],
   "id": "93140b5a926619f0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:44.586033Z",
     "start_time": "2025-11-18T23:49:44.581208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HungarianMatcher(nn.Module):\n",
    "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 2):\n",
    "        super(HungarianMatcher, self).__init__()\n",
    "\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "\n",
    "    def _get_index_map(self, targets):\n",
    "        batch_idx = torch.cat([torch.full((len(t['labels']),), i, dtype=torch.int64)\n",
    "                               for i, t in enumerate(targets)])\n",
    "        gt_idx = torch.cat([torch.arange(len(t['labels']), dtype=torch.int64)\n",
    "                            for t in targets])\n",
    "        return batch_idx, gt_idx\n",
    "\n",
    "    #Manhattan distance between predicted and GTBoxes\n",
    "\n",
    "    def _bbox_distance(self, pred_boxes, targets):\n",
    "        boxes = torch.cat([t['boxes'] for t in targets])\n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        cost = torch.cdist(pred_boxes, boxes, p=1)\n",
    "        return cost\n",
    "\n",
    "    def _giou_loss(self, pred_boxes, targets):\n",
    "        all_cost = []\n",
    "\n",
    "        for i, t in enumerate(targets):\n",
    "            preds = pred_boxes[i]\n",
    "            gts = t['boxes']\n",
    "\n",
    "            if len(gts) == 0:\n",
    "                all_cost.append(torch.zeros(preds.size(0), device=preds.device))\n",
    "                continue\n",
    "            pred_xyxy = cxcywh_to_xyxy(preds)\n",
    "            targetxyxy = cxcywh_to_xyxy(gts)\n",
    "\n",
    "            giou = generalized_box_iou(pred_xyxy, targetxyxy)\n",
    "            best_giou_per_pred = giou.max(dim=-1)[0]\n",
    "            all_cost.append(-best_giou_per_pred)\n",
    "        return all_cost\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        bs, num_queries = outputs['pred_logits'].shape[:2]\n",
    "        indices = []\n",
    "\n",
    "        for i in range(bs):\n",
    "            # --- extract predictions for this image ---\n",
    "            out_prob = outputs['pred_logits'][i].softmax(-1)  # [N, C]\n",
    "            out_bbox = outputs['pred_boxes'][i]  # [N, 4]\n",
    "\n",
    "            tgt_ids = targets[i]['labels']  # [K]\n",
    "            tgt_bbox = targets[i]['boxes']  # [K, 4]\n",
    "\n",
    "            # --- compute costs ---\n",
    "            cost_class = -out_prob[:, tgt_ids]  # [N, K]\n",
    "            cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)  # [N, K]\n",
    "            cost_giou = -generalized_box_iou(\n",
    "            cxcywh_to_xyxy(out_bbox), cxcywh_to_xyxy(tgt_bbox)\n",
    "            )  # [N, K]\n",
    "\n",
    "            # --- total cost ---\n",
    "            C = (self.cost_class * cost_class +\n",
    "                 self.cost_bbox * cost_bbox +\n",
    "                 self.cost_giou * cost_giou)\n",
    "\n",
    "            i_idx, j_idx = linear_sum_assignment(C.cpu())\n",
    "            indices.append((\n",
    "                torch.as_tensor(i_idx, dtype=torch.int64),\n",
    "                torch.as_tensor(j_idx, dtype=torch.int64)\n",
    "            ))\n",
    "\n",
    "        return indices\n",
    "\n"
   ],
   "id": "a43006b674fcc9a4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:44.597013Z",
     "start_time": "2025-11-18T23:49:44.590318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Understand the Set Criterion\n",
    "class SetCriterion(nn.Module):\n",
    "    def __init__(self, matcher, weight_dict, eos_coef=0.1, losses=None):\n",
    "        super(SetCriterion, self).__init__()\n",
    "        if losses is None:\n",
    "            losses = ['labels', 'boxes']\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "\n",
    "    def _get_src_batch_indices(self, indices):\n",
    "        batch_idx = torch.cat(\n",
    "            [torch.full((len(src),), i, dtype=torch.int64)\n",
    "             for i, (src, _) in enumerate(indices)]\n",
    "        )\n",
    "        src_idx = torch.cat(\n",
    "            [src for (src, _) in indices]\n",
    "        )\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute classification loss for matched predictions only\"\"\"\n",
    "        batch_idx, src_idx = self._get_src_batch_indices(indices)\n",
    "        pred_logits = outputs['pred_logits'][batch_idx, src_idx]\n",
    "        #Ground truth label for the same box\n",
    "        target_classes = torch.cat([t['labels'][J] for t, (_, J) in zip(targets, indices)])\n",
    "        #Compute loss function\n",
    "        loss_ce = F.cross_entropy(pred_logits, target_classes, reduction='none')\n",
    "        weights = torch.ones_like(target_classes, dtype=torch.float)\n",
    "        loss = (loss_ce * weights).sum() / num_boxes\n",
    "        return {'loss_ce': loss}\n",
    "\n",
    "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute DETR box regress loss\"\"\"\n",
    "        batch_idx, src_idx =self._get_src_batch_indices(indices)\n",
    "        src_boxes = outputs['pred_boxes'][batch_idx, src_idx]\n",
    "\n",
    "        #Get each box ground truth\n",
    "        target_boxes = torch.cat(\n",
    "            [t['boxes'][J] for t, (_, J) in zip(targets, indices)],\n",
    "        )\n",
    "\n",
    "         # L1 loss   numeric distance between boxes\n",
    "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
    "        loss_bbox = loss_bbox.sum()/ num_boxes\n",
    "\n",
    "        #GIOU Loss (Overlapping Boxes)\n",
    "        src_xyxy = cxcywh_to_xyxy(src_boxes)\n",
    "        target_xyxy = cxcywh_to_xyxy(target_boxes)\n",
    "\n",
    "        giou = generalized_box_iou(src_xyxy, target_xyxy)\n",
    "        giou = torch.nan_to_num(giou, nan= 0.0, posinf=0.0, neginf=-1.0)\n",
    "        loss_giou = (1 - torch.diag(giou)).sum() / num_boxes\n",
    "\n",
    "        return {'loss_box':loss_bbox, 'loss_giou': loss_giou}\n",
    "\n",
    "\n",
    "    def forward(self, outputs, targets ):\n",
    "        \"\"\"Compute the total DETR LOSS\"\"\"\n",
    "        indices = self.matcher(outputs, targets)\n",
    "        num_boxes = sum(len(t['labels']) for t in targets)\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device = next(iter(outputs.values())).device)\n",
    "        if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "            torch.distributed.all_reduce(num_boxes)\n",
    "            num_boxes = num_boxes / torch.distributed.get_world_size()\n",
    "        num_boxes = max(num_boxes.item(), 1.0)\n",
    "\n",
    "        losses ={}\n",
    "\n",
    "        for loss in self.losses:\n",
    "            if loss == 'labels':\n",
    "                losses.update(self.loss_labels(outputs, targets, indices, num_boxes))\n",
    "            elif loss == 'boxes':\n",
    "                losses.update(self.loss_boxes(outputs, targets, indices, num_boxes))\n",
    "        total_loss = sum(self.weight_dict[k] * v for k, v in losses.items() if k in self.weight_dict)\n",
    "        losses['total_loss'] = total_loss\n",
    "        return losses\n"
   ],
   "id": "a24d0d3738b0df2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:44.614510Z",
     "start_time": "2025-11-18T23:49:44.602854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = {\n",
    "    \"pred_logits\": torch.randn(2, 5, 91),  # 2 images, 5 queries each\n",
    "    \"pred_boxes\": torch.rand(2, 5, 4)\n",
    "}\n",
    "targets = [\n",
    "    {\"labels\": torch.tensor([3, 5, 7]), \"boxes\": torch.rand(3, 4)},\n",
    "    {\"labels\": torch.tensor([4, 8]), \"boxes\": torch.rand(2, 4)}\n",
    "]\n",
    "matcher = HungarianMatcher()\n",
    "indices = matcher(outputs, targets)\n",
    "print(indices)\n"
   ],
   "id": "29d51ce5e490bdf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([1, 2, 3]), tensor([0, 1, 2])), (tensor([1, 2]), tensor([0, 1]))]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:44.638023Z",
     "start_time": "2025-11-18T23:49:44.627567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weight_dict = {'loss_ce': 1.0, 'loss_bbox': 5.0, 'loss_giou': 2.0}\n",
    "outputs = {\n",
    "    \"pred_logits\": torch.randn(2, 5, 91),\n",
    "    \"pred_boxes\": torch.rand(2, 5, 4)\n",
    "}\n",
    "targets = [\n",
    "    {\"labels\": torch.tensor([3, 5, 7]), \"boxes\": torch.rand(3, 4)},\n",
    "    {\"labels\": torch.tensor([4, 8]), \"boxes\": torch.rand(2, 4)}\n",
    "]\n",
    "\n",
    "matcher = HungarianMatcher()\n",
    "criterion = SetCriterion(matcher, weight_dict, eos_coef=0.1, losses=['labels', 'boxes'])\n",
    "\n",
    "loss_dict = criterion(outputs, targets)\n",
    "total_loss = loss_dict['total_loss']\n",
    "print(\"Matched indices:\", matcher(outputs, targets))\n",
    "print(\"Total loss:\", total_loss)\n",
    "print(\"Loss breakdown:\", loss_dict)\n"
   ],
   "id": "ca96ec9756db8e97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched indices: [(tensor([0, 1, 3]), tensor([0, 2, 1])), (tensor([0, 1]), tensor([0, 1]))]\n",
      "Total loss: tensor(6.2435)\n",
      "Loss breakdown: {'loss_ce': tensor(4.8012), 'loss_box': tensor(0.5859), 'loss_giou': tensor(0.7211), 'total_loss': tensor(6.2435)}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:44.655026Z",
     "start_time": "2025-11-18T23:49:44.651502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, backbone_name = 'resnet50', dilation = False, return_layers = None):\n",
    "        super().__init__()\n",
    "        if return_layers is None:\n",
    "            return_layers = {'layer4': '0'}\n",
    "        backbone = getattr(models, backbone_name)(weights=ResNet50_Weights.IMAGENET1K_V1, replace_stride_with_dilation = [False, False, dilation])\n",
    "        self.body = backbone\n",
    "\n",
    "        if dilation:\n",
    "            self.body.layer4[0].downsample[0].stride = (1, 1)\n",
    "            self.body.layer4[0].conv2.stride = (1, 1)\n",
    "            self.body.layer4[2].conv1.stride = (1, 1)\n",
    "            self.body.layer4[2].conv2.stride = (1, 1)\n",
    "            self.body.layer4[2].downsample[0].stride = (1, 1)\n",
    "\n",
    "        self.out_channels = 2048 if backbone_name == 'resnet101' else 256 * 8\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.body.conv1(inputs)\n",
    "        x =self.body.bn1(x)\n",
    "        x = self.body.relu(x)\n",
    "        x = self.body.maxpool(x)\n",
    "        x = self.body.layer1(x)\n",
    "        x = self.body.layer2(x)\n",
    "        x = self.body.layer3(x)\n",
    "        x = self.body.layer4(x)\n",
    "        return x\n"
   ],
   "id": "5ef02543b6fe93f3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:44.671293Z",
     "start_time": "2025-11-18T23:49:44.667885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Detect layout: batch-first or sequence-first\n",
    "        if x.dim() == 3 and x.shape[0] < x.shape[1]:\n",
    "            # Likely (seq_len, batch, d_model)\n",
    "            seq_len = x.size(0)\n",
    "            return x + self.pe[:, :seq_len, :].transpose(0, 1)\n",
    "        else:\n",
    "            # Likely (batch, seq_len, d_model)\n",
    "            seq_len = x.size(1)\n",
    "            return x + self.pe[:, :seq_len, :]\n"
   ],
   "id": "bb7bfd1822b12172",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:44.680590Z",
     "start_time": "2025-11-18T23:49:44.675387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import math\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % nhead == 0\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.head_dim = d_model // nhead\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, attn_mask=None, key_padding_mask=None) -> Tensor:\n",
    "        # --- Detect layout (batch-first vs seq-first) ---\n",
    "        if query.dim() == 3 and query.shape[0] < query.shape[1]:\n",
    "            # seq-first layout (L, B, D)\n",
    "            query = query.transpose(0, 1)  # -> (B, L, D)\n",
    "            key = key.transpose(0, 1)\n",
    "            value = value.transpose(0, 1)\n",
    "            seq_first = True\n",
    "        else:\n",
    "            seq_first = False\n",
    "\n",
    "        B, tgt_len, d = query.shape\n",
    "        src_len = key.shape[1]\n",
    "\n",
    "        # --- Project Q, K, V ---\n",
    "        Q = self.q_proj(query).view(B, tgt_len, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(key).view(B, src_len, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(value).view(B, src_len, self.nhead, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # --- Attention weights ---\n",
    "        attn = (Q @ K.transpose(-2, -1)) / self.scale  # (B, nhead, tgt, src)\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # --- Weighted sum ---\n",
    "        out = (attn @ V).transpose(1, 2).contiguous().view(B, tgt_len, d)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        # --- Convert back if needed ---\n",
    "        if seq_first:\n",
    "            out = out.transpose(0, 1)  # -> (L, B, D)\n",
    "        return out\n"
   ],
   "id": "2315f235b81e160c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:44.691099Z",
     "start_time": "2025-11-18T23:49:44.686462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "#Opt for pytorch builtin transformer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class DETR(nn.Module):\n",
    "    def __init__(self, num_classes=20, num_queries=100, d_model=256, nhead=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6,\n",
    "                 dim_feedforward=2048, dropout=0.1, backbone='resnet50', dilation=False):\n",
    "        super().__init__()\n",
    "        self.num_queries = num_queries\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes + 1  # + âˆ…\n",
    "\n",
    "        # Backbone + projection\n",
    "        self.backbone = Backbone(backbone, dilation)\n",
    "        self.conv = nn.Conv2d(self.backbone.out_channels, d_model, 1)\n",
    "\n",
    "        # Positional encodings\n",
    "        self.encoder_pe = PositionalEncoding(d_model)\n",
    "        self.decoder_pe = PositionalEncoding(d_model, num_queries)\n",
    "\n",
    "        # Transformer (batch_first=True)\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward, dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model, nhead, dim_feedforward, dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "\n",
    "        # Object queries\n",
    "        self.query_embed = nn.Embedding(num_queries, d_model)\n",
    "\n",
    "        # Prediction heads\n",
    "        self.class_embed = nn.Linear(d_model, num_classes)\n",
    "        self.bbox_embed = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Criterion\n",
    "        matcher = HungarianMatcher(cost_class=1, cost_bbox=5, cost_giou=2)\n",
    "        weight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}\n",
    "        self.criterion = SetCriterion(matcher, weight_dict, eos_coef=0.1, losses=['labels', 'boxes'])\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # Backbone feature extraction\n",
    "        features = self.backbone(x)\n",
    "        features = self.conv(features)\n",
    "        B, C, H, W = features.shape\n",
    "        src = features.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Add positional encoding\n",
    "        src = self.encoder_pe(src)\n",
    "\n",
    "        # Transformer encoder\n",
    "        memory = self.transformer_encoder(src)\n",
    "\n",
    "        # Prepare queries\n",
    "        query_embed = self.query_embed.weight.unsqueeze(0).repeat(B, 1, 1)\n",
    "        tgt = torch.zeros_like(query_embed)\n",
    "        query_embed = self.decoder_pe(query_embed)\n",
    "\n",
    "        # Transformer decoder\n",
    "        hs = self.transformer_decoder(tgt, memory)\n",
    "\n",
    "        # Predictions\n",
    "        outputs_class = self.class_embed(hs)\n",
    "        outputs_coord = self.bbox_embed(hs)\n",
    "        out = {'pred_logits': outputs_class, 'pred_boxes': outputs_coord}\n",
    "\n",
    "        if self.training and targets is not None:\n",
    "            loss_dict = self.criterion(out, targets)\n",
    "            return loss_dict\n",
    "\n",
    "        return out\n"
   ],
   "id": "e56670aeba735951",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:47.811395Z",
     "start_time": "2025-11-18T23:49:44.696116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = DETR(num_classes=20)\n",
    "x = torch.randn(2, 3, 800, 1333)\n",
    "out = model(x)\n",
    "print(out['pred_logits'].shape, out['pred_boxes'].shape)\n"
   ],
   "id": "451654c424ac1243",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 20]) torch.Size([2, 100, 4])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:57.198690Z",
     "start_time": "2025-11-18T23:49:56.281979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"huanghanchina/pascal-voc-2012\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "id": "1f7e1c394c29b839",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
      "Path to dataset files: C:\\Users\\modam\\.cache\\kagglehub\\datasets\\huanghanchina\\pascal-voc-2012\\versions\\1\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:57.215632Z",
     "start_time": "2025-11-18T23:49:57.210659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    max_h = max(img.shape[1] for img in images)\n",
    "    max_w = max(img.shape[2] for img in images)\n",
    "    padded_images = []\n",
    "    for img in images:\n",
    "        pad_h = max_h - img.shape[1]\n",
    "        pad_w = max_w - img.shape[2]\n",
    "        padded_img = F.pad(img, (0, pad_w, 0, pad_h))\n",
    "        padded_images.append(padded_img)\n",
    "\n",
    "    # Stack into a single tensor\n",
    "    images = torch.stack(padded_images, 0)\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "def build_voc_dataset(image_set='train', transforms=None):\n",
    "    path = r\"C:\\Users\\modam\\.cache\\kagglehub\\datasets\\huanghanchina\\pascal-voc-2012\\versions\\1\"\n",
    "    dataset = datasets.VOCDetection(root = path, year='2012', image_set=image_set, download=False, transform=transforms)\n",
    "    return dataset\n",
    "\n",
    "def voc_transforms():\n",
    "    return transforms.Compose([\n",
    "    transforms.Resize((480, 480)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "# For crop aug: Custom collate with random crop p=0.5\n",
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_set, transforms):\n",
    "        self.dataset = build_voc_dataset(image_set, transforms)\n",
    "        self.crop_prob = 0.5\n",
    "\n",
    "\n",
    "        self.VOC_CLASSES = [\n",
    "            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "            'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant',\n",
    "            'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.dataset[idx]\n",
    "        ann = target['annotation']\n",
    "        objects = ann['object']\n",
    "        if not isinstance(objects, list):\n",
    "            objects = [objects]\n",
    "\n",
    "        boxes, labels = [], []\n",
    "        for obj in objects:\n",
    "            bndbox = obj['bndbox']\n",
    "            xmin = float(bndbox['xmin'])\n",
    "            ymin = float(bndbox['ymin'])\n",
    "            xmax = float(bndbox['xmax'])\n",
    "            ymax = float(bndbox['ymax'])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(self.VOC_CLASSES.index(obj['name']))\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # Handle both PIL and Tensor\n",
    "        if hasattr(img, \"size\") and not callable(img.size):\n",
    "            width, height = img.size  # PIL image\n",
    "        else:\n",
    "            _, height, width = img.shape  # Tensor (C, H, W)\n",
    "\n",
    "        boxes /= torch.tensor([width, height, width, height], dtype=torch.float32)\n",
    "\n",
    "        target = {'boxes': boxes, 'labels': labels}\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ],
   "id": "b31d987e6d4eecec",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T23:49:57.276680Z",
     "start_time": "2025-11-18T23:49:57.219516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "v_ds = VOCDataset(image_set='train', transforms=voc_transforms())\n",
    "img, target = v_ds[0]\n",
    "print(img.shape)\n",
    "print(target['boxes'].shape)\n",
    "print(target['labels'].shape)\n",
    "print(target)\n"
   ],
   "id": "d6c4d3ac16dc244c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 480, 480])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2])\n",
      "{'boxes': tensor([[0.1104, 0.1813, 0.9812, 0.8750],\n",
      "        [0.3292, 0.0917, 0.6021, 0.3479]]), 'labels': tensor([12, 14])}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T04:21:11.108441Z",
     "start_time": "2025-11-19T04:21:11.072950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data\n",
    "dataset_train = VOCDataset('trainval', voc_transforms())\n",
    "dataset_val = VOCDataset('val', voc_transforms())\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(sampler_train, batch_size = 64, drop_last=True)\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train, collate_fn=collate_fn)\n",
    "data_loader_val = DataLoader(dataset_val,batch_size = 64, shuffle=False, collate_fn=collate_fn)"
   ],
   "id": "bff432d1411a236c",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T04:21:12.128996Z",
     "start_time": "2025-11-19T04:21:11.815646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for images, targets in data_loader_val:\n",
    "    print(images.shape)              # batch of images\n",
    "    print(len(targets))              # number of targets (== batch size)\n",
    "    print(targets[0].keys())         # see what's inside one target dict\n",
    "    print(targets[0]['boxes'].shape) # number of boxes in first image\n",
    "    break\n"
   ],
   "id": "2969ff29b9aa11f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 480, 480])\n",
      "64\n",
      "dict_keys(['boxes', 'labels'])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T04:21:14.869651Z",
     "start_time": "2025-11-19T04:21:14.865437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.gamma = gamma\n",
    "        self.T_i = T_0\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [base_lr + (self.eta_max - base_lr) * self.T_cur / self.T_up for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) *\n",
    "                    (1 + math.cos(math.pi * (self.T_cur - self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = self.T_i * self.T_mult\n",
    "                self.eta_max = self.eta_max * self.gamma\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                self.cycle = n\n",
    "                self.T_i = self.T_0 * self.T_mult ** n\n",
    "                self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                self.eta_max = self.base_eta_max * (self.gamma ** n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                self.eta_max = self.base_eta_max\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n"
   ],
   "id": "f66f8472cabd86f4",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T04:21:15.943117Z",
     "start_time": "2025-11-19T04:21:15.936944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, device, epoch, print_freq=50, clip_norm=0.1, scheduler = None):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        cleaned_targets = []\n",
    "        for t in targets:\n",
    "            boxes = t[\"boxes\"].clamp(0, 1)\n",
    "            x1, y1 = torch.min(boxes[:, 0], boxes[:, 2]), torch.min(boxes[:, 1], boxes[:, 3])\n",
    "            x2, y2 = torch.max(boxes[:, 0], boxes[:, 2]), torch.max(boxes[:, 1], boxes[:, 3])\n",
    "            boxes = torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "            if torch.isnan(boxes).any():\n",
    "                print(\"Skipped sample with NaN boxes.\")\n",
    "                continue\n",
    "\n",
    "            cleaned_targets.append({\n",
    "                \"boxes\": boxes.to(device),\n",
    "                \"labels\": t[\"labels\"].to(device)\n",
    "            })\n",
    "\n",
    "        if not cleaned_targets:\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, cleaned_targets)\n",
    "        total_loss = sum(loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        if clip_norm > 0:\n",
    "            clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "        if (i + 1) % print_freq == 0:\n",
    "            print(f\"[Epoch {epoch+1}] Step {i+1}/{len(data_loader)} | \"\n",
    "                  f\"Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(data_loader)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nEpoch {epoch+1} completed | Avg Loss: {avg_loss:.4f} | Time: {elapsed:.2f}s\\n\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    print(\"\\nEvaluating...\")\n",
    "    for images, targets in data_loader:\n",
    "        images = images.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        outputs = model(images)\n",
    "        loss_dict = model.criterion(outputs, targets)\n",
    "        val_loss = sum(loss_dict.values()).item()\n",
    "        total_loss += val_loss\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\\n\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train_detr(model, train_loader, val_loader, optimizer, device,\n",
    "               num_epochs=10, clip_norm=0.1, scheduler = None, save_path=\"detr_checkpoint.pth\"):\n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device, epoch, clip_norm=clip_norm)\n",
    "        val_loss = evaluate(model, val_loader, device)\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}: LR = {scheduler.get_lr()[0]:.6f}\")\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"val_loss\": best_val,\n",
    "            }, save_path)\n",
    "            print(f\" Saved best model (Val Loss: {val_loss:.4f})\\n\")\n",
    "\n",
    "    print(f\"Training complete. Best Validation Loss: {best_val:.4f}\")\n"
   ],
   "id": "3cf230ad68da61be",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-19T04:21:16.560440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = DETR(num_classes=20).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingWarmUpRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,\n",
    "    T_mult=2,\n",
    "    eta_max=1e-4,\n",
    "    T_up=3,\n",
    "    gamma=0.8\n",
    ")\n",
    "train_detr(model, data_loader_train, data_loader_val, optimizer, device,\n",
    "           num_epochs=10, clip_norm=0.1, scheduler=scheduler)"
   ],
   "id": "9640ff643ec6eb68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6fae4f5f428e00e9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
